{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data = pd.read_csv('u.data', encoding='latin1', header=0, engine='python',sep='\\t', names=[\"User_Id\", \"Movie_Id\", \"Rating\", \"Timestamp\"])\n",
    "items_data = pd.read_csv('u.item',encoding='latin1', header=0, engine='python',sep='|', names=[\"Movie_Id\",\"movie_title\",\"release_date\",\"videorelease_date\",\"IMDb_URL\",\"unknown\",\"Action\",\"Adventure\",\"Animation\",\"Children\",\"Comedy\",\"Crime\",\"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"])\n",
    "users_data = pd.read_csv('user.user',encoding='latin1', header=0, engine='python',sep='|', names=[\"User_Id\",\"age\",\"gender\",\"occupation\",\"zipcode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.DataFrame(ratings_data)\n",
    "ratings_df = ratings_df.drop('Timestamp', axis=1)\n",
    "items_df = pd.DataFrame(items_data)\n",
    "items_df = items_df.drop(['release_date', 'videorelease_date',  'unknown'], axis=1)\n",
    "users_df = pd.DataFrame(users_data)\n",
    "users_df = users_df.drop(['zipcode','occupation'],axis=1)\n",
    "#items_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_df=items_df.drop(['Movie_Id', 'IMDb_URL'], axis=1)\n",
    "genre_matrix=genre_df.values\n",
    "#genre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_df = pd.merge(ratings_df, users_df, on='User_Id')\n",
    "#combined_df = pd.merge(combined_df, items_df, on='Movie_Id')\n",
    "#print(combined_df)\n",
    "#print(combined_df.columns)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = ratings_df.pivot(index='User_Id', columns='Movie_Id', values='Rating')\n",
    "user_item_matrix.fillna(0, inplace=True)\n",
    "#print(user_item_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Negative Matrix Factorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import NMF\n",
    "#import numpy as np\n",
    "#from scipy.sparse.linalg import svds\n",
    "#from scipy.sparse import csr_matrix\n",
    "\n",
    "# Assuming user_item_matrix is defined\n",
    "\n",
    "# Convert user-item matrix to CSR format\n",
    "#matrix_sparse_csr = csr_matrix(user_item_matrix)\n",
    "\n",
    "# Number of latent factors\n",
    "#k = 10\n",
    "\n",
    "# Apply NMF\n",
    "#model = NMF(n_components=k, init='random', random_state=42)\n",
    "#user_factors = model.fit_transform(matrix_sparse_csr)\n",
    "#item_factors = model.components_\n",
    "\n",
    "# Reconstruct the user-item matrix\n",
    "#final_matrix = np.dot(user_factors, item_factors)\n",
    "#print(final_matrix)\n",
    "#print(type(final_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.25204675e+00  2.08644700e+00  1.29105941e+00 ... -8.39539471e-03\n",
      "   1.98410372e-02  5.78772121e-02]\n",
      " [ 1.85298728e+00 -1.07304097e-01  1.63140719e-01 ...  1.05689177e-02\n",
      "   2.40161236e-04 -1.41019252e-04]\n",
      " [-1.16700091e-01 -1.98725527e-02  9.20469305e-02 ...  1.96975747e-02\n",
      "  -3.17732077e-03 -1.03339447e-02]\n",
      " ...\n",
      " [ 2.01284547e+00 -2.21248849e-02  2.95242272e-01 ... -8.51598317e-04\n",
      "  -3.53967691e-03 -4.23170973e-03]\n",
      " [ 1.37042937e+00  1.17361446e-01 -4.41472736e-01 ...  1.44812394e-02\n",
      "   2.00441121e-03 -2.93361945e-02]\n",
      " [ 1.73635347e+00  2.07782144e+00  1.01422641e+00 ... -9.34285128e-03\n",
      "   2.62328960e-02  2.75075243e-02]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "matrix_sparse_csr = csr_matrix(user_item_matrix)\n",
    "\n",
    "k = 10\n",
    "\n",
    "u, s, vt = svds(matrix_sparse_csr, k=k)\n",
    "\n",
    "final_matrix = np.dot(u, np.dot(np.diag(s), vt))\n",
    "print(final_matrix)\n",
    "print(type(final_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 3. 4. ... 0. 0. 0.]\n",
      " [4. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [5. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 5. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "user_item_matrix_np = user_item_matrix.values\n",
    "\n",
    "print(user_item_matrix_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.00000000e+00  3.00000000e+00  4.00000000e+00 ... -8.39539471e-03\n",
      "   1.98410372e-02  5.78772121e-02]\n",
      " [ 4.00000000e+00 -1.07304097e-01  1.63140719e-01 ...  1.05689177e-02\n",
      "   2.40161236e-04 -1.41019252e-04]\n",
      " [-1.16700091e-01 -1.98725527e-02  9.20469305e-02 ...  1.96975747e-02\n",
      "  -3.17732077e-03 -1.03339447e-02]\n",
      " ...\n",
      " [ 5.00000000e+00 -2.21248849e-02  2.95242272e-01 ... -8.51598317e-04\n",
      "  -3.53967691e-03 -4.23170973e-03]\n",
      " [ 1.37042937e+00  1.17361446e-01 -4.41472736e-01 ...  1.44812394e-02\n",
      "   2.00441121e-03 -2.93361945e-02]\n",
      " [ 1.73635347e+00  5.00000000e+00  1.01422641e+00 ... -9.34285128e-03\n",
      "   2.62328960e-02  2.75075243e-02]]\n"
     ]
    }
   ],
   "source": [
    "zero_indices = np.where(user_item_matrix_np == 0)\n",
    "\n",
    "user_item_matrix_updated = user_item_matrix_np.copy()\n",
    "\n",
    "user_item_matrix_updated[zero_indices] = final_matrix[zero_indices]\n",
    "\n",
    "print(user_item_matrix_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Total NUmber of Zeros Replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeros in the original matrix: 1486126\n",
      "Number of values replaced: 1486126\n"
     ]
    }
   ],
   "source": [
    "num_zeros_original = np.sum(user_item_matrix_np == 0)\n",
    "\n",
    "user_item_matrix_updated = user_item_matrix_np.copy()\n",
    "\n",
    "zero_indices = np.where(user_item_matrix_np == 0)\n",
    "\n",
    "user_item_matrix_updated[zero_indices] = final_matrix[zero_indices]\n",
    "\n",
    "num_replaced = np.sum(user_item_matrix_np != user_item_matrix_updated)\n",
    "\n",
    "print(\"Number of zeros in the original matrix:\", num_zeros_original)\n",
    "print(\"Number of values replaced:\", num_replaced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing The Original Array With Min and Max Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array: [[ 5.00000000e+00  3.00000000e+00  4.00000000e+00 ... -8.39539471e-03\n",
      "   1.98410372e-02  5.78772121e-02]\n",
      " [ 4.00000000e+00 -1.07304097e-01  1.63140719e-01 ...  1.05689177e-02\n",
      "   2.40161236e-04 -1.41019252e-04]\n",
      " [-1.16700091e-01 -1.98725527e-02  9.20469305e-02 ...  1.96975747e-02\n",
      "  -3.17732077e-03 -1.03339447e-02]\n",
      " ...\n",
      " [ 5.00000000e+00 -2.21248849e-02  2.95242272e-01 ... -8.51598317e-04\n",
      "  -3.53967691e-03 -4.23170973e-03]\n",
      " [ 1.37042937e+00  1.17361446e-01 -4.41472736e-01 ...  1.44812394e-02\n",
      "   2.00441121e-03 -2.93361945e-02]\n",
      " [ 1.73635347e+00  5.00000000e+00  1.01422641e+00 ... -9.34285128e-03\n",
      "   2.62328960e-02  2.75075243e-02]]\n",
      "Normalized array: [[4.38940913 3.55026279 3.96983596 ... 2.2880208  2.29986805 2.31582701]\n",
      " [3.96983596 2.24652136 2.35999275 ... 2.29597772 2.29164405 2.29148412]\n",
      " [2.24257906 2.28320529 2.33016371 ... 2.29980786 2.29021016 2.28720744]\n",
      " ...\n",
      " [4.38940913 2.28226028 2.41541902 ... 2.29118598 2.29005813 2.28976777]\n",
      " [2.86653868 2.340785   2.10631317 ... 2.29761922 2.29238428 2.2792346 ]\n",
      " [3.02007061 4.38940913 2.71708547 ... 2.28762327 2.3025499  2.3030847 ]]\n"
     ]
    }
   ],
   "source": [
    "normalized_array = (user_item_matrix_updated - np.min(user_item_matrix_updated)) / (np.max(user_item_matrix_updated) - np.min(user_item_matrix_updated))\n",
    "min_range = 1\n",
    "max_range = 5\n",
    "scaled_array = min_range + normalized_array * (max_range - min_range)\n",
    "print(\"Original array:\", user_item_matrix_updated)\n",
    "print(\"Normalized array:\", scaled_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjuscent Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 1682)\n",
      "[[ 1.          0.31800653  0.3264873  ... -0.12827065  0.04740076\n",
      "   0.0500615 ]\n",
      " [ 0.31800653  1.          0.36543637 ... -0.04584622  0.13629031\n",
      "   0.12954874]\n",
      " [ 0.3264873   0.36543637  1.         ... -0.04303403  0.03002813\n",
      "   0.14269135]\n",
      " ...\n",
      " [-0.12827065 -0.04584622 -0.04303403 ...  1.         -0.01086667\n",
      "  -0.022534  ]\n",
      " [ 0.04740076  0.13629031  0.03002813 ... -0.01086667  1.\n",
      "   0.02446696]\n",
      " [ 0.0500615   0.12954874  0.14269135 ... -0.022534    0.02446696\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaled_array is assumed to be your dataset\n",
    "\n",
    "# Standardize the array\n",
    "scaled_array = StandardScaler().fit_transform(scaled_array)\n",
    "\n",
    "# Compute the mean of each item\n",
    "item_means = scaled_array.mean(axis=0)\n",
    "\n",
    "# Compute the adjusted scaled array by subtracting the mean from each item\n",
    "adjusted_scaled_array = scaled_array - item_means\n",
    "\n",
    "# Compute the adjusted cosine similarity matrix\n",
    "item_similarity_matrix = cosine_similarity(adjusted_scaled_array.T)\n",
    "\n",
    "print(item_similarity_matrix.shape)\n",
    "print(item_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import pearsonr\n",
    "\n",
    " #scaled_array is assumed to be your dataset\n",
    "\n",
    " #Calculate Pearson correlation matrix\n",
    "#correlation_matrix = np.corrcoef(scaled_array.T)\n",
    "\n",
    "#print(correlation_matrix.shape)\n",
    "#print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import rankdata\n",
    "\n",
    "# scaled_array is assumed to be your dataset\n",
    "\n",
    "# Rank the data along each column\n",
    "#ranked_data = np.apply_along_axis(rankdata, 0, scaled_array)\n",
    "\n",
    "# Calculate Spearman correlation matrix\n",
    "#item_similarity_matrix = np.corrcoef(ranked_data.T)\n",
    "\n",
    "#print(item_similarity_matrix.shape)\n",
    "#print(item_similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proximity Impact Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.25615056e-15 -2.34995245e-15  7.21138714e-15 ...  9.90667428e-14\n",
      "  9.35772021e-16  3.49734381e-14]\n",
      "(1682,)\n",
      "[[ 0.5         0.15900326  0.16324365 ... -0.06413533  0.02370038\n",
      "   0.02503075]\n",
      " [ 0.15900326  0.5         0.18271818 ... -0.02292311  0.06814515\n",
      "   0.06477437]\n",
      " [ 0.16324365  0.18271818  0.5        ... -0.02151701  0.01501407\n",
      "   0.07134568]\n",
      " ...\n",
      " [-0.06413533 -0.02292311 -0.02151701 ...  0.5        -0.00543333\n",
      "  -0.011267  ]\n",
      " [ 0.02370038  0.06814515  0.01501407 ... -0.00543333  0.5\n",
      "   0.01223348]\n",
      " [ 0.02503075  0.06477437  0.07134568 ... -0.011267    0.01223348\n",
      "   0.5       ]]\n",
      "(1682, 1682)\n"
     ]
    }
   ],
   "source": [
    "popularity_ratings = np.mean(scaled_array, axis=0)\n",
    "print(popularity_ratings)\n",
    "print(popularity_ratings.shape)\n",
    "similarity_weight = 0.5\n",
    "popularity_weight = 0.5\n",
    "\n",
    "weighted_similarity_matrix = similarity_weight * item_similarity_matrix + popularity_weight * popularity_ratings[:, np.newaxis]\n",
    "print(weighted_similarity_matrix)\n",
    "print(weighted_similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding All Similar Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_similar_items(weighted_similarity_matrix):\n",
    "    num_items = weighted_similarity_matrix.shape[0]\n",
    "    all_similar_items_dict = {}\n",
    "\n",
    "    for item_index in range(num_items):\n",
    "        similarity_scores = weighted_similarity_matrix[item_index, :]\n",
    "        \n",
    "        similar_item_indices = np.argsort(similarity_scores)[::-1] + 1\n",
    "\n",
    "        all_similar_items_dict[item_index + 1] = similar_item_indices\n",
    "\n",
    "    return all_similar_items_dict\n",
    "\n",
    "all_similar_items_dict = find_all_similar_items(weighted_similarity_matrix)\n",
    "\n",
    "#for item_index, similar_items in all_similar_items_dict.items():\n",
    "    #print(f\"Item {item_index}: All Similar Items {similar_items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie Recommendation Sytem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Recommendations for Movie 'Toy Story (1995)': ['Willy Wonka and the Chocolate Factory (1971)', 'Independence Day (ID4) (1996)', 'Star Wars (1977)', 'Return of the Jedi (1983)', 'Mission: Impossible (1996)']\n"
     ]
    }
   ],
   "source": [
    "def recommend_top_items(all_similar_items_dict, num_recommendations=15):\n",
    "    top_recommendations_dict = {}\n",
    "\n",
    "    for item_index, similar_items in all_similar_items_dict.items():\n",
    "        \n",
    "        recommended_items = [item for item in similar_items if item != item_index][:num_recommendations]\n",
    "\n",
    "        top_recommendations_dict[item_index] = recommended_items\n",
    "\n",
    "    return top_recommendations_dict\n",
    "\n",
    "try:\n",
    "    user_input = int(input(\"Enter movie ID to get recommendations: \"))\n",
    "except ValueError:\n",
    "    print(\"Invalid input. Please enter a valid integer.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "if user_input not in all_similar_items_dict:\n",
    "    print(f\"Invalid movie ID: {user_input}\")\n",
    "else:\n",
    "    top_recommendations_dict = recommend_top_items(all_similar_items_dict, num_recommendations=5)\n",
    "    movie_title = items_df.loc[user_input - 1, 'movie_title']\n",
    "    recommended_movie_titles = [items_df.loc[rec - 1, 'movie_title'] for rec in top_recommendations_dict[user_input]]\n",
    "\n",
    "    print(f\"Top 5 Recommendations for Movie '{movie_title}': {recommended_movie_titles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widgets for Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb049e5b2181433d98946a43ceb9483c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select a Movie:', options=('-- Select Movie --', 'Toy Story (1995)', 'GoldenEye (1995)',â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create select box for movie titles\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create select box for movie titles\n",
    "movie_titles = ['-- Select Movie --'] + items_df['movie_title'].tolist()\n",
    "movie_title_dropdown = widgets.Dropdown(options=movie_titles, description='Select a Movie:', value='-- Select Movie --')\n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        movie_title = change['new']\n",
    "        if movie_title != '-- Select Movie --':\n",
    "            movie_index = items_df.index[items_df['movie_title'] == movie_title].tolist()[0] + 1\n",
    "            recommendations = recommend_top_items(all_similar_items_dict)[movie_index]\n",
    "            recommended_movie_titles = [items_df.loc[rec - 1, 'movie_title'] for rec in recommendations]\n",
    "            print(f\"Top 5 Recommendations for Movie '{movie_title}': {recommended_movie_titles}\")\n",
    "\n",
    "movie_title_dropdown.observe(on_change)\n",
    "\n",
    "display(movie_title_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "\n",
    "#with open('user_item_matrix.pkl', 'wb') as f:\n",
    "    #pickle.dump(user_item_matrix, f)\n",
    "\n",
    "#with open('items_df.pkl', 'wb') as f:\n",
    "    #pickle.dump(items_df, f)\n",
    "\n",
    "#with open('all_similar_items_dict.pkl', 'wb') as f:\n",
    "    #pickle.dump(all_similar_items_dict, f)\n",
    "\n",
    "#with open('top_recommendations_dict.pkl', 'wb') as f:\n",
    "    #pickle.dump(top_recommendations_dict, f)\n",
    "\n",
    "# Save the model and necessary data structures into a pickle file\n",
    "#with open('recommender_system.pickle', 'wb') as file:\n",
    "    #pickle.dump({\n",
    "        #'model': model,\n",
    "        #'user_item_matrix': user_item_matrix,\n",
    "        #'item_similarity_matrix': item_similarity_matrix,\n",
    "        #'popularity_ratings': popularity_ratings,\n",
    "        #'combined_df' : combined_df,\n",
    "        #'items_df':items_df\n",
    "    #}, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Matrix:\n",
      "[[ 1.73652926  1.06684381  0.06700046 -0.13941483  1.01586054]\n",
      " [-0.09294322  0.06868232  0.96755329  0.8231338  -0.13857591]\n",
      " [ 0.82289664  0.89933773  3.07210321  2.48756553  0.2177702 ]\n",
      " [ 0.41348549  0.79311719  4.17815991  3.46269282 -0.11900574]\n",
      " [ 4.39807669  2.71159368  0.24394459 -0.29072683  2.56641355]]\n",
      "Matrix Type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# User-Item Matrix\n",
    "user_item_matrix = np.array([\n",
    "    [3, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 2, 0],\n",
    "    [0, 2, 5, 0, 0],\n",
    "    [1, 0, 3, 5, 0],\n",
    "    [4, 3, 0, 0, 3]\n",
    "], dtype=float)  # Cast the matrix to float data type\n",
    "\n",
    "# Convert to CSR matrix\n",
    "matrix_sparse_csr = csr_matrix(user_item_matrix)\n",
    "#print(matrix_sparse_csr)\n",
    "# Number of singular values to compute\n",
    "k = 2\n",
    "\n",
    "# Perform SVD\n",
    "u, s, vt = svds(matrix_sparse_csr, k=k)\n",
    "\n",
    "# Reconstruct the matrix\n",
    "final_matrix = np.dot(u, np.dot(np.diag(s), vt))\n",
    "\n",
    "# Print the reconstructed matrix\n",
    "print(\"Reconstructed Matrix:\")\n",
    "print(final_matrix)\n",
    "print(\"Matrix Type:\", type(final_matrix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions NLTK for Fetching Posters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "def fetch_poster_from_tmdb_by_title(movie_title):\n",
    "    api_key = 'eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJmMjEwZWEzOGJjNDhkZTQyZDQ1YzMzMzUxMGZkODI1MyIsInN1YiI6IjY1ZmQzMTFmMzc4MDYyMDE3ZTg3MWM2ZCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.ifEhqFnSL7MLwhmIbn7mMN0fwRdD6DmWTwmvkQpZqQQ'\n",
    "    url = \"https://api.themoviedb.org/3/search/movie\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    # Extract title and year from the input string using regular expressions\n",
    "    match = re.match(r'^(.*?)\\s*\\((\\d{4})\\)$', movie_title)\n",
    "    if match:\n",
    "        title, year = match.groups()\n",
    "    else:\n",
    "        title = movie_title\n",
    "        year = None\n",
    "    \n",
    "    params = {\n",
    "        \"query\": title,\n",
    "        \"year\": year,  # Add year as a parameter\n",
    "        \"include_adult\": False,\n",
    "        \"language\": \"en-US\",\n",
    "        \"page\": 1\n",
    "    }\n",
    "    print(\"API Request Params:\", params)\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    print(\"API Response:\", response.json())\n",
    "    data = response.json()\n",
    "    if 'results' in data and data['results']:\n",
    "        poster_path = data['results'][0].get('poster_path')\n",
    "        if poster_path:\n",
    "            base_url = \"https://image.tmdb.org/t/p/w500\"  # Adjust the size as needed\n",
    "            poster_url = f\"{base_url}{poster_path}\"\n",
    "            return poster_url\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making A Streamlit Web App Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 20:02:29.930 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\moham\\OneDrive\\Desktop\\pro\\venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "def main():\n",
    "    # Assuming you have items_df loaded\n",
    "    selected_movie = st.selectbox(\"Select a Movie:\", items_df['movie_title'].values)\n",
    "\n",
    "    if st.button('Show Recommendations'):\n",
    "        # Check if selected_movie is not '-- Select Movie --'\n",
    "        if selected_movie != '-- Select Movie --':\n",
    "            # Find movie_id corresponding to the selected movie title\n",
    "            movie_id_candidates = items_df.loc[items_df['movie_title'] == selected_movie, 'Movie_Id'].values\n",
    "            if len(movie_id_candidates) > 0:\n",
    "                # If at least one movie_id is found, use the first one\n",
    "                movie_id = movie_id_candidates[0]\n",
    "                all_similar_items_dict = find_all_similar_items(weighted_similarity_matrix)\n",
    "                recommendations = recommend_top_items(all_similar_items_dict)[movie_id]\n",
    "                recommended_movie_titles = [items_df.loc[rec - 1, 'movie_title'] for rec in recommendations]\n",
    "                st.write(f\"Top 5 Recommendations for Movie '{selected_movie}': {recommended_movie_titles}\")\n",
    "                for title in recommended_movie_titles:\n",
    "                    try:\n",
    "                        poster_url = fetch_poster_from_tmdb_by_title(title)\n",
    "                    except KeyError:\n",
    "                        poster_url = None\n",
    "\n",
    "                    if poster_url is not None:\n",
    "                        st.image(poster_url, caption=title, use_column_width=True)\n",
    "                    else:\n",
    "                        st.write(f\"No poster available for {title}\")\n",
    "            else:\n",
    "                # If no movie_id is found, display a message\n",
    "                st.write(f\"No movie found with the title '{selected_movie}'.\")\n",
    "        else:\n",
    "            # If '-- Select Movie --' is selected, display a message\n",
    "            st.write(\"Please select a movie.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
